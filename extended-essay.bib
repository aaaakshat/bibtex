Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Vanian2019,
author = {Vanian, Jonathan},
journal = {Fortune},
month = {may},
title = {{Keep Your A.I. Buzzwords Straight}},
url = {https://fortune.com/2019/05/28/ai-buzzwords/},
year = {2019}
}
@article{Biswas,
abstract = {Sentiment analysis is a well researched natural language processing field. It is a challenging machine learning task due to the recursive nature of sentences, different length of documents and sarcasm. Traditional approaches to sentiment analysis use count or frequency of words in the text which are assigned sentiment value by some expert. These approaches disregard the order of words and the complex meanings they can convey. Gated Recurrent Units are recent form of recurrent neural network which have the ability to store information of long term dependencies in sequential data. In this work we showed that GRU are suitable for processing long textual data and applied it to the task of sentiment analysis. We showed its effectiveness by comparing with tf-idf and word2vec models. We also showed that GRUs are faster in convergence than LSTM, another gating network. We applied a number of modifications to the standard GRU to make it train faster and yet less prone to over training. We found the better performimg hyperparameters of the GRU-net through extensive cross-validation testing. Finally we ensembled the best performing GRU models for even better performance.},
author = {Biswas, Shamim and Chadda, Ekamber and Ahmad, Faiyaz},
file = {::},
issn = {2393-9915},
journal = {Advances in Computer Science and Information Technology (ACSIT)},
keywords = {Sentiment analysis,ensemble,gated recurrent unit,kaggle},
month = {apr},
number = {11},
pages = {59--63},
title = {{Sentiment Analysis with Gated Recurrent Units}},
url = {http://www.krishisanskriti.org/acsit.html},
volume = {2}
}
@misc{Clement2020,
author = {Clement, J.},
booktitle = {Statista},
month = {jun},
title = {{Global digital population as of April 2020}},
url = {https://www.statista.com/statistics/617136/digital-population-worldwide/},
urldate = {2020-06-07},
year = {2020}
}
@techreport{Yin2017,
abstract = {Deep neural networks (DNNs) have revolutionized the field of natural language processing (NLP). Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state-of-the-art on many NLP tasks often switches due to the battle of CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.},
archivePrefix = {arXiv},
arxivId = {1702.01923v1},
author = {Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"{u}}tzeSch, Hinrich and Munich, Lmu},
eprint = {1702.01923v1},
file = {::},
pages = {7},
title = {{Comparative Study of CNN and RNN for Natural Language Processing}},
year = {2017}
}
