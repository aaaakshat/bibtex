Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Su2018,
author = {Su, Yuanhang and Kuo, C},
doi = {10.1016/j.neucom.2019.04.044},
journal = {Neurocomputing},
title = {{On Extended Long Short-term Memory and Dependent Bidirectional Recurrent Neural Network}},
volume = {356},
year = {2018}
}
@techreport{Buhrmester2019,
abstract = {Deep Learning is a state-of-the-art technique to make inference on extensive or complex data. As a black box model due to their multilayer nonlinear structure, Deep Neural Networks are often criticized to be non-transparent and their predictions not traceable by humans. Furthermore, the models learn from artificial datasets, often with bias or contaminated discriminating content. Through their increased distribution, decision-making algorithms can contribute promoting prejudge and unfairness which is not easy to notice due to lack of transparency. Hence, scientists developed several so-called explanators or explainers which try to point out the connection between input and output to represent in a simplified way the inner structure of machine learning black boxes. In this survey we differ the mechanisms and properties of explaining systems for Deep Neural Networks for Computer Vision tasks. We give a comprehensive overview about taxonomy of related studies and compare several survey papers that deal with explainability in general. We work out the drawbacks and gaps and summarize further research ideas.},
address = {Ettlingen},
archivePrefix = {arXiv},
arxivId = {1911.12116v1},
author = {Buhrmester, Vanessa and M{\"{u}}nch, David and Arens, Michael},
eprint = {1911.12116v1},
file = {::},
institution = {Fraunhofer IOSB},
keywords = {Black Box,Deep Neural Network,Ethics,Explainable AI,Explainer,Explanator,Interpretability,Trust},
month = {nov},
title = {{Analysis of Explainers of Black Box Deep Neural Networks for Computer Vision: A Survey}},
year = {2019}
}
@article{article,
author = {Alghifari, M F and Gunawan, Teddy and Kartiwi, Mira},
doi = {10.11591/ijeecs.v10.i2.pp554-561},
journal = {Indonesian Journal of Electrical Engineering and Computer Science},
pages = {554--561},
title = {{Speech emotion recognition using deep feedforward neural network}},
volume = {10},
year = {2018}
}
@misc{JeanneDugan1996,
author = {{Jeanne Dugan}, I.},
booktitle = {Bloomberg},
month = {oct},
title = {{`The Internet Is The Great Equalizer'}},
url = {https://www.bloomberg.com/news/articles/1996-10-20/the-internet-is-the-great-equalizer},
urldate = {2020-06-07},
year = {1996}
}
@article{owidinternet,
annote = {https://ourworldindata.org/internet},
author = {{Max Roser}, Hannah Ritchie and Ortiz-Ospina, Esteban},
journal = {Our World in Data},
title = {{Internet}},
year = {2015}
}
@article{Biswas,
abstract = {Sentiment analysis is a well researched natural language processing field. It is a challenging machine learning task due to the recursive nature of sentences, different length of documents and sarcasm. Traditional approaches to sentiment analysis use count or frequency of words in the text which are assigned sentiment value by some expert. These approaches disregard the order of words and the complex meanings they can convey. Gated Recurrent Units are recent form of recurrent neural network which have the ability to store information of long term dependencies in sequential data. In this work we showed that GRU are suitable for processing long textual data and applied it to the task of sentiment analysis. We showed its effectiveness by comparing with tf-idf and word2vec models. We also showed that GRUs are faster in convergence than LSTM, another gating network. We applied a number of modifications to the standard GRU to make it train faster and yet less prone to over training. We found the better performimg hyperparameters of the GRU-net through extensive cross-validation testing. Finally we ensembled the best performing GRU models for even better performance.},
author = {Biswas, Shamim and Chadda, Ekamber and Ahmad, Faiyaz},
file = {::},
issn = {2393-9915},
journal = {Advances in Computer Science and Information Technology (ACSIT)},
keywords = {Sentiment analysis,ensemble,gated recurrent unit,kaggle},
month = {apr},
number = {11},
pages = {59--63},
title = {{Sentiment Analysis with Gated Recurrent Units}},
url = {http://www.krishisanskriti.org/acsit.html},
volume = {2}
}
@article{11239260720160201,
abstract = {The use of new technologies along with the popularity of social networks has given the power of anonymity to the users. The ability to create an alter-ego with no relation to the actual user, creates a situation in which no one can certify the match between a profile and a real person. This problem generates situations, repeated daily, in which users with fake accounts, or at least not related to their real identity, publish news, reviews or multimedia material trying to discredit or attack other people who may or may not be aware of the attack. These acts can have great impact on the affected victims' environment generating situations in which virtual attacks escalate intofatal consequences in real life. In this article, we present a methodology to detect and associate fake profiles on Twitter social network which are employed for defamatory activities to a real profile within the same network by analysing the content of the comments generated by both profiles. Accompanying this appr},
author = {GAL{\'{A}}N-GARC{\'{I}}A, PATXI and {DE LA PUERTA}, JOS{\'{E}} GAVIRIA and G{\'{O}}MEZ, CARLOS LAORDEN and SANTOS, IGOR and BRINGAS, PABLO GARC{\'{I}}A},
issn = {13670751},
journal = {Logic Journal of the IGPL},
keywords = {Cyberbullying,Machine learning,Online social networks,Online trolling,Twitter (Web resource),cyberbullying,information retrieval,trolling},
number = {1},
pages = {42--53},
title = {{Supervised machine learning for the detection of troll profiles in twitter social network: application to a real case of cyberbullying.}},
url = {https://search.ebscohost.com/login.aspx?direct=true{\&}AuthType=ip,shib{\&}db=bth{\&}AN=112392607{\&}site=eds-live{\&}custid=s5027800},
volume = {24},
year = {2016}
}
@article{article,
author = {Lipton, Zachary},
title = {{A Critical Review of Recurrent Neural Networks for Sequence Learning}},
year = {2015}
}
@article{Vandebosch2008DefiningCA,
author = {Vandebosch, Heidi and Cleemput, Katrien Van},
journal = {Cyberpsychology {\&} behavior : the impact of the Internet, multimedia and virtual reality on behavior and society},
pages = {499--503},
title = {{Defining Cyberbullying: A Qualitative Research into the Perceptions of Youngsters}},
volume = {11 4},
year = {2008}
}
@article{Krzyk2018,
author = {Krzyk, Kamil},
journal = {Medium},
title = {{Coding Deep Learning for Beginners â€” Linear Regression (Part 3): Training with Gradient Descent}},
url = {https://towardsdatascience.com/coding-deep-learning-for-beginners-linear-regression-gradient-descent-fcd5e0fc077d},
year = {2018}
}
@misc{Edwards,
author = {Edwards, April and Edwards, Lynne and {D. Davison}, Brian},
title = {{ChatCoder}},
url = {https://www.chatcoder.com/},
urldate = {2020-06-10}
}
@misc{Hern2019,
author = {Hern, Alex},
booktitle = {The Guardian},
month = {sep},
title = {{Revealed: catastrophic effects of working as a Facebook moderator | Technology | The Guardian}},
url = {https://www.theguardian.com/technology/2019/sep/17/revealed-catastrophic-effects-working-facebook-moderator},
urldate = {2020-06-07},
year = {2019}
}
@misc{Clement2020,
author = {Clement, J.},
booktitle = {Statista},
month = {jun},
title = {{Global digital population as of April 2020}},
url = {https://www.statista.com/statistics/617136/digital-population-worldwide/},
urldate = {2020-06-07},
year = {2020}
}
@article{Sharma2017,
author = {Sharma, Sagar},
journal = {Towards Data Science},
title = {{Epoch vs Batch Size vs Iterations}},
url = {https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9},
year = {2017}
}
@misc{Chotiner2019,
author = {Chotiner, Isaac},
booktitle = {The New Yorker},
month = {jul},
title = {{The Underworld of Online Content Moderation}},
url = {https://www.newyorker.com/news/q-and-a/the-underworld-of-online-content-moderation},
urldate = {2020-06-08},
year = {2019}
}
@techreport{EnyinnaNwankpa2018,
abstract = {Deep neural networks have been successfully used in diverse emerging domains to solve real world complex problems with may more deep learning(DL) architectures, being developed to date. To achieve these state-of-the-art performances, the DL architectures use activation functions (AFs), to perform diverse computations between the hidden layers and the output layers of any given DL architecture. This paper presents a survey on the existing AFs used in deep learning applications and highlights the recent trends in the use of the activation functions for deep learning applications. The novelty of this paper is that it compiles majority of the AFs used in DL and outlines the current trends in the applications and usage of these functions in practical deep learning deployments against the state-of-the-art research results. This compilation will aid in making effective decisions in the choice of the most suitable and appropriate activation function for any given application, ready for deployment. This paper is timely because most research papers on AF highlights similar works and results while this paper will be the first, to compile the trends in AF applications in practice against the research results from literature, found in deep learning research to date.},
archivePrefix = {arXiv},
arxivId = {1811.03378v1},
author = {{Enyinna Nwankpa}, Chigozie and Ijomah, Winifred and Gachagan, Anthony and Marshall, Stephen},
eprint = {1811.03378v1},
file = {::},
keywords = {Index Terms activation function,activation function choices,activation function types,deep learning,learning algorithms,neural networks},
month = {nov},
title = {{Activation Functions: Comparison of Trends in Practice and Research for Deep Learning}},
year = {2018}
}
@techreport{Raschka2020,
abstract = {Smarter applications are making better use of the insights gleaned from data, having an impact on every industry and research discipline. At the core of this revolution lies the tools and the methods that are driving it, from processing the massive piles of data generated each day to learning from and taking useful action. Deep neural networks, along with advancements in classical ML and scalable general-purpose GPU computing, have become critical components of artificial intelligence, enabling many of these astounding breakthroughs and lowering the barrier to adoption. Python continues to be the most preferred language for scientific computing, data science, and machine learning, boosting both performance and productivity by enabling the use of low-level libraries and clean high-level APIs. This survey offers insight into the field of machine learning with Python, taking a tour through important topics to identify some of the core hardware and software paradigms that have enabled it. We cover widely-used libraries and concepts, collected together for holistic comparison, with the goal of educating the reader and driving the field of Python machine learning forward.},
archivePrefix = {arXiv},
arxivId = {2002.04803v2},
author = {Raschka, Sebastian and Patterson, Joshua and Nolet, Corey},
eprint = {2002.04803v2},
file = {::},
keywords = {GPU computing,Python,data science,deep learning,machine learning,neural networks},
month = {mar},
title = {{Machine Learning in Python: Main developments and technology trends in data science, machine learning, and artificial intelligence}},
url = {http://packages.pypy.org},
year = {2020}
}
@misc{W.Patchin2019,
author = {{W. Patchin}, Justin},
booktitle = {Cyberbullying Research Center},
month = {jul},
title = {{Summary of Our Cyberbullying Research (2004-2016)}},
url = {https://cyberbullying.org/summary-of-our-cyberbullying-research},
urldate = {2020-06-07},
year = {2019}
}
@article{Vanian2019,
author = {Vanian, Jonathan},
journal = {Fortune},
month = {may},
title = {{Keep Your A.I. Buzzwords Straight}},
url = {https://fortune.com/2019/05/28/ai-buzzwords/},
year = {2019}
}
@misc{Copeland2016,
author = {Copeland, Michael},
booktitle = {Nvidia Blogs},
month = {jul},
title = {{The Difference Between AI, Machine Learning, and Deep Learning? | NVIDIA Blog}},
url = {https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/},
urldate = {2020-06-08},
year = {2016}
}
@misc{Ko2019,
abstract = {This project uses a Gated Recurrent Unit implemented in Keras to perform sentiment analysis on movie reviews},
address = {Singapore},
author = {Ko, Johann},
month = {jul},
title = {{GitHub - johann95ko/gru-sentiment-analysis}},
url = {https://github.com/johann95ko/gru-sentiment-analysis},
year = {2019}
}
@article{10.1371/journal.pone.0203794,
abstract = {While social media offer great communication opportunities, they also increase the vulnerability of young people to threatening situations online. Recent studies report that cyberbullying constitutes a growing problem among youngsters. Successful prevention depends on the adequate detection of potentially harmful messages and the information overload on the Web requires intelligent systems to identify potential risks automatically. The focus of this paper is on automatic cyberbullying detection in social media text by modelling posts written by bullies, victims, and bystanders of online bullying. We describe the collection and fine-grained annotation of a cyberbullying corpus for English and Dutch and perform a series of binary classification experiments to determine the feasibility of automatic cyberbullying detection. We make use of linear support vector machines exploiting a rich feature set and investigate which information sources contribute the most for the task. Experiments on a hold-out test set reveal promising results for the detection of cyberbullying-related posts. After optimisation of the hyperparameters, the classifier yields an F1 score of 64{\%} and 61{\%} for English and Dutch respectively, and considerably outperforms baseline systems.},
author = {{Van Hee}, Cynthia and Jacobs, Gilles and Emmery, Chris and Desmet, Bart and Lefever, Els and Verhoeven, Ben and {De Pauw}, Guy and Daelemans, Walter and Hoste, V{\'{e}}ronique},
doi = {10.1371/journal.pone.0203794},
journal = {PLOS ONE},
number = {10},
pages = {1--22},
publisher = {Public Library of Science},
title = {{Automatic detection of cyberbullying in social media text}},
url = {https://doi.org/10.1371/journal.pone.0203794},
volume = {13},
year = {2018}
}
@techreport{McCarthy1955,
author = {McCarthy, J. and Minsky, M. and Rochester, N. and Shannon, C.E.},
file = {::},
institution = {Dartmouth College},
month = {aug},
title = {{A Proposal for the Dartmouth Summer Research Project on Artificial Intelligence}},
year = {1955}
}
@article{Bao2017,
author = {Bao, Wei and Yue, Jun and Rao, Yulei},
doi = {10.1371/journal.pone.0180944},
journal = {PLoS ONE},
title = {{A deep learning framework for financial time series using stacked autoencoders and long-short term memory}},
volume = {12},
year = {2017}
}
@article{13317416020181027,
abstract = {The article discusses the development of an artificial intelligence (AI) machine learning algorithm to prevent cyberbullying of young people on social media, referencing an article coauthored by scientist Gilles Jacobs in the "PLoS One" journal.},
author = {Klein, Alice},
issn = {02624079},
journal = {New Scientist},
keywords = {COMPUTER algorithms,MACHINE learning,PREVENTION of cyberbullying,SOCIAL media,YOUTH},
number = {3201},
pages = {13},
title = {{Al may protect kids from cyberbullies.}},
url = {https://search.ebscohost.com/login.aspx?direct=true{\&}AuthType=ip,shib{\&}db=a9h{\&}AN=133174160{\&}site=eds-live{\&}custid=s5027800},
year = {2018}
}
@article{Goltsman2017,
author = {Goltsman, Kirill},
journal = {Data Science Foundation},
month = {sep},
title = {{Introduction to Artificial Neural Networks (ANNs)}},
url = {https://datascience.foundation/sciencewhitepaper/introduction-to-artificial-neural-networks-(anns)},
year = {2017}
}
@article{9988848520141201,
abstract = {State-of-the-art studies on cyberbullying detection, using text classification, predominantly take it for granted that streaming text can be completely labelled. However, the rapid growth of unlabelled data generated in real time from online content renders this virtually impossible. In this paper, we propose a session-based framework for automatic detection of cyberbullying within the large volume of unlabelled streaming text. Given that the streaming data from Social Networks arrives in large volume at the server system, we incorporate an ensemble of one-class classifiers in the session-based framework. System uses Multi-Agent distributed environment to process streaming data from multiple social network sources. The proposed strategy tackles real world situations, where only a few positive instances of cyberbullying are available for initial training. Our main contribution in this paper is to automatically detect cyberbullying in real world situations, where labelled data is not re},
author = {Nahar, Vinita and Li, Xue and Zhang, Hao Lan and Pang, Chaoyi},
issn = {15701263},
journal = {Web Intelligence {\&} Agent Systems},
keywords = {CYBERBULLYING,Cyberbullying detection,DATA analysis,MULTIAGENT systems,ONLINE social networks,SUPERVISED learning (Machine learning),multi-agent,positive unlabelled learning,semi-supervised learning,social networks,text-stream classification},
number = {4},
pages = {375--388},
title = {{Detecting cyberbullying in social networks using multi-agent system.}},
url = {https://search.ebscohost.com/login.aspx?direct=true{\&}AuthType=ip,shib{\&}db=a9h{\&}AN=99888485{\&}site=eds-live{\&}custid=s5027800},
volume = {12},
year = {2014}
}
@article{Ahmad2017,
abstract = {Energy prediction models are used in buildings as a performance evaluation engine in advanced control and optimisation, and in making informed decisions by facility managers and utilities for enhanced energy efficiency. Simplified and data-driven models are often the preferred option where pertinent information for detailed simulation are not available and where fast responses are required. We compared the performance of the widely-used feed-forward back-propagation artificial neural network (ANN) with random forest (RF), an ensemble-based method gaining popularity in prediction â€“ for predicting the hourly HVAC energy consumption of a hotel in Madrid, Spain. Incorporating social parameters such as the numbers of guests marginally increased prediction accuracy in both cases. Overall, ANN performed marginally better than RF with root-mean-square error (RMSE) of 4.97 and 6.10 respectively. However, the ease of tuning and modelling with categorical variables offers ensemble-based algorithms an advantage for dealing with multi-dimensional complex data, typical in buildings. RF performs internal cross-validation (i.e. using out-of-bag samples) and only has a few tuning parameters. Both models have comparable predictive power and nearly equally applicable in building energy applications.},
author = {Ahmad, Muhammad Waseem and Mourshed, Monjur and Rezgui, Yacine},
doi = {10.1016/j.enbuild.2017.04.038},
file = {::},
issn = {03787788},
journal = {Energy and Buildings},
keywords = {Artificial neural networks,Data mining,Decision trees,Energy efficiency,Ensemble algorithms,HVAC systems,Random forest},
month = {jul},
pages = {77--89},
publisher = {Elsevier Ltd},
title = {{Trees vs Neurons: Comparison between random forest and ANN for high-resolution prediction of building energy consumption}},
volume = {147},
year = {2017}
}
@article{Sherstinsky2020,
abstract = {Because of their effectiveness in broad practical applications, LSTM networks have received a wealth of coverage in scientific journals, technical blogs, and implementation guides. However, in most articles, the inference formulas for the LSTM network and its parent, RNN, are stated axiomatically, while the training formulas are omitted altogether. In addition, the technique of "unrolling" an RNN is routinely presented without justification throughout the literature. The goal of this tutorial is to explain the essential RNN and LSTM fundamentals in a single document. Drawing from concepts in Signal Processing, we formally derive the canonical RNN formulation from differential equations. We then propose and prove a precise statement, which yields the RNN unrolling technique. We also review the difficulties with training the standard RNN and address them by transforming the RNN into the "Vanilla LSTM" 1 network through a series of logical arguments. We provide all equations pertaining to the LSTM system together with detailed descriptions of its constituent entities. Albeit unconventional, our choice of notation and the method for presenting the LSTM system emphasizes ease of understanding. As part of the analysis, we identify new opportunities to enrich the LSTM system and incorporate these extensions into the Vanilla LSTM network, producing the most general LSTM variant to date. The target reader has already been exposed to RNNs and LSTM networks through numerous available resources and is open to an alternative pedagogical approach. A Machine Learning practitioner seeking guidance for implementing our new augmented LSTM model in software for experimentation and research will find the insights and derivations in this treatise valuable as well.},
archivePrefix = {arXiv},
arxivId = {1808.03314v7},
author = {Sherstinsky, Alex},
eprint = {1808.03314v7},
file = {::},
journal = {Physica D: Nonlinear Phenomena},
month = {mar},
number = {Machine Learning and Dynamical Systems},
title = {{Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network}},
url = {https://www.linkedin.com/in/alexsherstinsky},
volume = {404},
year = {2020}
}
@article{Gombru2018,
author = {Bruballa, Ra{\'{u}}l G{\'{o}}mez},
title = {{Understanding Categorical Cross-Entropy Loss, Binary Cross-Entropy Loss, Softmax Loss, Logistic Loss, Focal Loss and all those confusing names}},
url = {https://gombru.github.io/2018/05/23/cross{\_}entropy{\_}loss/},
year = {2018}
}
@article{Md_2013,
author = {Md, Ali Hossain and Md, Mijanur Rahman and {Kumar Prodhan}, Uzzal and Md, Farukuzzaman Khan},
doi = {10.5121/ijist.2013.3401},
issn = {2249-1139},
journal = {International Journal of Information Sciences and Techniques},
month = {jul},
number = {4},
pages = {1--9},
publisher = {Academy and Industry Research Collaboration Center (AIRCC)},
title = {{Implementation of back-propagation neural Network for isolated bangla speech recognition}},
url = {http://dx.doi.org/10.5121/ijist.2013.3401},
volume = {3},
year = {2013}
}
@inproceedings{6147681,
abstract = {Cyber bullying is the use of technology as a medium to bully someone. Although it has been an issue for many years, the recognition of its impact on young people has recently increased. Social networking sites provide a fertile medium for bullies, and teens and young adults who use these sites are vulnerable to attacks. Through machine learning, we can detect language patterns used by bullies and their victims, and develop rules to automatically detect cyber bullying content. The data we used for our project was collected from the website Formspring.me, a question-and-answer formatted website that contains a high percentage of bullying content. The data was labeled using a web service, Amazon's Mechanical Turk. We used the labeled data, in conjunction with machine learning techniques provided by the Weka tool kit, to train a computer to recognize bullying content. Both a C4.5 decision tree learner and an instance-based learner were able to identify the true positives with 78.5{\%} accuracy.},
author = {Reynolds, K and Kontostathis, A and Edwards, L},
booktitle = {2011 10th International Conference on Machine Learning and Applications and Workshops},
doi = {10.1109/ICMLA.2011.152},
keywords = {learning (artificial intelligence);social networki},
month = {dec},
pages = {241--244},
title = {{Using Machine Learning to Detect Cyberbullying}},
volume = {2},
year = {2011}
}
@techreport{Cawley2010,
abstract = {Model selection strategies for machine learning algorithms typically involve the numerical opti-misation of an appropriate model selection criterion, often based on an estimator of generalisation performance, such as k-fold cross-validation. The error of such an estimator can be broken down into bias and variance components. While unbiasedness is often cited as a beneficial quality of a model selection criterion, we demonstrate that a low variance is at least as important, as a non-negligible variance introduces the potential for over-fitting in model selection as well as in training the model. While this observation is in hindsight perhaps rather obvious, the degradation in performance due to over-fitting the model selection criterion can be surprisingly large, an observation that appears to have received little attention in the machine learning literature to date. In this paper, we show that the effects of this form of over-fitting are often of comparable magnitude to differences in performance between learning algorithms, and thus cannot be ignored in empirical evaluation. Furthermore, we show that some common performance evaluation practices are susceptible to a form of selection bias as a result of this form of over-fitting and hence are unreliable. We discuss methods to avoid over-fitting in model selection and subsequent selection bias in performance evaluation, which we hope will be incorporated into best practice. While this study concentrates on cross-validation based model selection, the findings are quite general and apply to any model selection practice involving the optimisation of a model selection criterion evaluated over a finite sample of data, including maximisation of the Bayesian evidence and optimisation of performance bounds.},
address = {Norwich},
author = {Cawley, Gavin C and Talbot, Nicola L C},
booktitle = {Journal of Machine Learning Research},
file = {::},
institution = {University of East Anglica},
keywords = {bias-variance trade-off,model selection,over-fitting,performance evaluation,selection bias},
month = {oct},
pages = {2079--2107},
title = {{On Over-fitting in Model Selection and Subsequent Selection Bias in Performance Evaluation}},
volume = {11},
year = {2010}
}
@techreport{Yin2017,
abstract = {Deep neural networks (DNNs) have revolutionized the field of natural language processing (NLP). Convolutional Neural Network (CNN) and Recurrent Neural Network (RNN), the two main types of DNN architectures, are widely explored to handle various NLP tasks. CNN is supposed to be good at extracting position-invariant features and RNN at modeling units in sequence. The state-of-the-art on many NLP tasks often switches due to the battle of CNNs and RNNs. This work is the first systematic comparison of CNN and RNN on a wide range of representative NLP tasks, aiming to give basic guidance for DNN selection.},
archivePrefix = {arXiv},
arxivId = {1702.01923v1},
author = {Yin, Wenpeng and Kann, Katharina and Yu, Mo and Sch{\"{u}}tzeSch, Hinrich and Munich, Lmu},
eprint = {1702.01923v1},
file = {::},
pages = {7},
title = {{Comparative Study of CNN and RNN for Natural Language Processing}},
year = {2017}
}
